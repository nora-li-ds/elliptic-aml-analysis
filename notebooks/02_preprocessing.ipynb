{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elliptic AML Dataset Exploration\n",
    "Initial exploration and visualization of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_RAW = Path(\"../data/elliptic_bitcoin_dataset\")\n",
    "DATA_INTERIM = Path(\"../data/interim\")      \n",
    "DATA_PROCESSED = Path(\"../data/processed\")  # for modelling\n",
    "for p in [DATA_INTERIM, DATA_PROCESSED]: p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载与完整性校验\n",
    "校验三件套：elliptic_txs_features.csv, elliptic_txs_edgelist.csv, elliptic_txs_classes.csv \n",
    "检查：形状、重复 txId、空值、取值域；edges_df 的节点是否都在 classes_df 里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(DATA_RAW/\"elliptic_txs_features.csv\", header=None)\n",
    "edges    = pd.read_csv(DATA_RAW/\"elliptic_txs_edgelist.csv\")\n",
    "classes  = pd.read_csv(DATA_RAW/\"elliptic_txs_classes.csv\")\n",
    "\n",
    "assert features.shape[0] == classes.shape[0]\n",
    "assert classes['txId'].is_unique\n",
    "\n",
    "# 空值/重复\n",
    "nulls = features.isna().sum().sum()\n",
    "dups_edges = edges.duplicated().sum()\n",
    "print(f\"NULLs in features: {nulls}, duplicate edges: {dups_edges}\")\n",
    "\n",
    "# 边的端点是否都在类表中\n",
    "assert set(edges['txId1']).issubset(set(classes['txId']))\n",
    "assert set(edges['txId2']).issubset(set(classes['txId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 标签清洗与映射\n",
    "\t•\t建议：保留三类问题两套数据：\n",
    "A) 二分类（丢弃 unknown）→ 给监督学习\n",
    "B) 半监督/异常检测（unknown 保留）→ 给 One-Class/IF/GNN 半监督\n",
    "\t•\t建议映射：{'illicit':1, 'licit':0}，unknown 先丢或单独存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并 txId\n",
    "features['txId'] = classes['txId']\n",
    "\n",
    "label_map = {'1':1, 1:1, '2':0, 2:0, 'unknown':np.nan}\n",
    "y_raw = classes['class'].map(label_map)\n",
    "\n",
    "mask_sup = y_raw.notna()\n",
    "y_sup = y_raw[mask_sup].astype(int)\n",
    "\n",
    "# 仅监督学习子集\n",
    "feat_sup = features.loc[mask_sup].reset_index(drop=True)\n",
    "y_sup = y_sup.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 基础特征工程与清洗\n",
    "\t•\t丢掉常数/近零方差特征\n",
    "\t•\t可选：对高缺失/极端值做处理（你的 features 通常无缺失）\n",
    "\t•\t共线筛选（>0.98 的相关性只保留其一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除 txId 外的原始数值列\n",
    "num_cols = [c for c in feat_sup.columns if c not in ['txId']]\n",
    "\n",
    "# 常数列\n",
    "nunique = feat_sup[num_cols].nunique()\n",
    "keep = nunique[nunique > 1].index.tolist()\n",
    "feat_sup = feat_sup[['txId'] + keep]\n",
    "\n",
    "# 高相关去重\n",
    "corr = feat_sup[keep].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.98)]\n",
    "feat_sup = feat_sup.drop(columns=to_drop)\n",
    "print(f\"dropped {len(to_drop)} highly correlated columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 时序信息与切分\n",
    "\n",
    "Elliptic 数据自带 time step（通常在 features 第 0 列），建议基于时间切分：\n",
    "\t•\t训练：前 70% 的时间步\n",
    "\t•\t验证：中间 10%\n",
    "\t•\t测试：最后 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常 features 第0列为 timestep（请确认你的版本）\n",
    "timestep_col = 0\n",
    "df = feat_sup.copy()\n",
    "df['timestep'] = df[timestep_col]\n",
    "\n",
    "# 对齐标签\n",
    "df['label'] = y_sup\n",
    "\n",
    "# 排序后切分\n",
    "df = df.sort_values('timestep').reset_index(drop=True)\n",
    "ts = df['timestep'].values\n",
    "\n",
    "t1 = np.quantile(ts, 0.7)\n",
    "t2 = np.quantile(ts, 0.8)\n",
    "\n",
    "train_idx = df['timestep'] <= t1\n",
    "valid_idx = (df['timestep'] > t1) & (df['timestep'] <= t2)\n",
    "test_idx  = df['timestep'] > t2\n",
    "\n",
    "train, valid, test = df[train_idx], df[valid_idx], df[test_idx]\n",
    "print(train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 标准化/缩放（仅在训练集拟合）（Code）\n",
    "\t•\t强烈建议用 sklearn 的 Pipeline 保存 scaler，并只用训练集拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in ['txId','label','timestep']]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = train[feature_cols].copy()\n",
    "X_valid = valid[feature_cols].copy()\n",
    "X_test  = test[feature_cols].copy()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_valid_s = scaler.transform(X_valid)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "y_train = train['label'].values\n",
    "y_valid = valid['label'].values\n",
    "y_test  = test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 类别不平衡处理（思路）\n",
    "\t•\t基线：模型里用 class_weight='balanced'\n",
    "\t•\t采样：如果要用 SMOTE/Under-sampling，只在训练集做\n",
    "\t•\t替代：Focal Loss（在后续 03_modeling/深度模型中再上）\n",
    "\n",
    "在预处理阶段先不做采样，避免把选择“写死”到数据层；把采样放到建模 Pipeline 更灵活。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把可训练矩阵与对照表、scaler 参数、列名、切分边界全部存起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 保存矩阵（推荐 parquet 或 npy）\n",
    "pd.DataFrame(X_train_s, columns=feature_cols).to_parquet(DATA_PROCESSED/\"X_train.parquet\")\n",
    "pd.DataFrame(X_valid_s, columns=feature_cols).to_parquet(DATA_PROCESSED/\"X_valid.parquet\")\n",
    "pd.DataFrame(X_test_s,  columns=feature_cols).to_parquet(DATA_PROCESSED/\"X_test.parquet\")\n",
    "\n",
    "pd.Series(y_train).to_frame(\"label\").to_parquet(DATA_PROCESSED/\"y_train.parquet\")\n",
    "pd.Series(y_valid).to_frame(\"label\").to_parquet(DATA_PROCESSED/\"y_valid.parquet\")\n",
    "pd.Series(y_test).to_frame(\"label\").to_parquet(DATA_PROCESSED/\"y_test.parquet\")\n",
    "\n",
    "# 保存切分用到的 txId，方便做图/回溯\n",
    "train[['txId','timestep']].to_parquet(DATA_INTERIM/\"train_ids.parquet\")\n",
    "valid[['txId','timestep']].to_parquet(DATA_INTERIM/\"valid_ids.parquet\")\n",
    "test[['txId','timestep']].to_parquet(DATA_INTERIM/\"test_ids.parquet\")\n",
    "\n",
    "# 保存 scaler 与元数据\n",
    "joblib.dump(scaler, DATA_PROCESSED/\"scaler.joblib\")\n",
    "meta = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"dropped_high_corr\": to_drop,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"timestep_quantiles\": {\"train<=q0.7\": float(t1), \"valid<=q0.8\": float(t2)},\n",
    "    \"samples\": {\"train\": int(len(train)), \"valid\": int(len(valid)), \"test\": int(len(test))}\n",
    "}\n",
    "with open(DATA_PROCESSED/\"metadata.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 质量闸（QA）与可复现性\n",
    "\t•\t检查各 split 标签比例\n",
    "\t•\t检查没有数据泄漏（验证/测试与训练的时间边界）\n",
    "\t•\t固定随机种子；记录依赖版本（pip freeze > requirements.txt）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, y in [(\"train\", y_train), (\"valid\", y_valid), (\"test\", y_test)]:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(name, dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
